# Two-Stage Fine-Tuning Training Configuration
# Stage 1: Freeze backbone, train heads only
# Stage 2: Unfreeze and fine-tune entire model

# Stage 1: Frozen backbone
stage1:
  epochs: 5
  lr: 1.0e-4

# Stage 2: Fine-tuning
stage2:
  epochs: 15
  lr: 1.0e-5

# Total epochs = stage1.epochs + stage2.epochs = 20

# DataLoader settings
batch_size: 8
num_workers: 4

# Mixed precision training
use_amp: true

# Optimizer
optimizer:
  name: AdamW
  weight_decay: 0.01

# Checkpoint settings
save_best_only: true
metric: score  # Save based on RÂ² score, not loss

# Loss configuration
loss:
  type: SmoothL1
  beta: 1.0
  weights:
    total: 0.50
    gdm: 0.20
    green: 0.10

