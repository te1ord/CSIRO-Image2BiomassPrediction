{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "552d499f",
   "metadata": {
    "papermill": {
     "duration": 0.002543,
     "end_time": "2025-11-10T21:14:54.096501",
     "exception": false,
     "start_time": "2025-11-10T21:14:54.093958",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# Methodology: CSIRO Pasture Biomass Prediction\n",
    "\n",
    "## 1. Core Strategy: Predicting Key Components\n",
    "\n",
    "The primary goal is to predict five biomass targets. Based on exploratory data analysis (EDA), we identified linear dependencies:\n",
    "* `Dry_Total_g` $\\approx$ `Dry_Green_g` + `Dry_Dead_g` + `Dry_Clover_g`\n",
    "* `GDM_g` $\\approx$ `Dry_Green_g` + `Dry_Clover_g`\n",
    "\n",
    "To avoid redundancy, the model is trained to predict only the **three most visually distinct and/or highest-weighted targets**:\n",
    "* `Dry_Total_g` (50% of the score)\n",
    "* `GDM_g` (20% of the score)\n",
    "* `Dry_Green_g` (10% of the score)\n",
    "\n",
    "The remaining two targets (`Dry_Dead_g` and `Dry_Clover_g`) are then **calculated during validation and inference** using subtraction (e.g., `pred_Clover = max(0, pred_GDM - pred_Green)`).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Data Handling & K-Fold Strategy\n",
    "\n",
    "* **Image Input:** All source images are high-resolution (`2000x1000` pixels).\n",
    "* **Two-Stream Processing:** To preserve fine-grained details (like clover leaves) that would be lost by resizing the entire image, the `Dataset` class crops each image into two `1000x1000` patches (a \"left\" and \"right\" half).\n",
    "* **High-Resolution Input:** Each `1000x1000` patch is then resized to **`768x768`**, maintaining a high level of detail.\n",
    "* **K-Fold Strategy:** We use a **5-Fold Cross-Validation** strategy due to the small dataset (357 images).\n",
    "* **Robust Splitting (GroupKFold):** To prevent data leakage (where similar images from the same day are in both train and validation), we use `GroupKFold` grouped by `Sampling_Date`. This ensures the model is validated on dates it has never seen.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Model Architecture: Two-Stream, Multi-Head\n",
    "\n",
    "The model uses a \"Two-Stream, Multi-Head\" architecture.\n",
    "* **Shared Backbone:** A single `timm` backbone (e.g., `convnext_tiny`) with pre-trained ImageNet weights is used.\n",
    "* **Two-Stream Input:**\n",
    "    * `img_left` $\\rightarrow$ `backbone` $\\rightarrow$ `features_left`\n",
    "    * `img_right` $\\rightarrow$ (same) `backbone` $\\rightarrow$ `features_right`\n",
    "* **Fusion:** The two feature vectors are concatenated: `combined_features = torch.cat([features_left, features_right])`.\n",
    "* **Multi-Head Output:** This combined vector is fed into **three separate, specialized MLP heads** (one for each target: `head_total`, `head_gdm`, `head_green`) to allow for task specialization.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Data Augmentation\n",
    "\n",
    "To compensate for the small dataset, augmentations are applied **independently** to the `img_left` and `img_right` patches.\n",
    "* `HorizontalFlip (p=0.5)`\n",
    "* `VerticalFlip (p=0.5)`\n",
    "* `RandomRotate90 (p=0.5)` (Only 90-degree rotations)\n",
    "* `ColorJitter`\n",
    "\n",
    "This independent application creates a much larger variety of training combinations.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Loss Function: Weighted SmoothL1Loss\n",
    "\n",
    "The model is optimized using a custom weighted loss function that aligns with the competition's scoring metric.\n",
    "* **Base Loss:** `nn.SmoothL1Loss` (Huber Loss) is used instead of `MSELoss` to make training more stable and less sensitive to outliers.\n",
    "* **Weighted Sum:** The final loss is a weighted sum of the individual losses, using the competition's scoring weights:\n",
    "    $$Loss = (0.5 \\cdot Loss_{Total}) + (0.2 \\cdot Loss_{GDM}) + (0.1 \\cdot Loss_{Green})$$\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Training Strategy: Two-Stage Fine-Tuning\n",
    "\n",
    "A two-stage \"Freeze/Unfreeze\" strategy is used to stabilize training on the small dataset.\n",
    "* **Stage 1 (Freeze):**\n",
    "    * **Epochs:** 1-5\n",
    "    * **Action:** The entire `backbone` is frozen. Only the three MLP heads are trained.\n",
    "    * **LR:** `1e-4`\n",
    "* **Stage 2 (Unfreeze/Fine-Tuning):**\n",
    "    * **Epochs:** 6-20\n",
    "    * **Action:** The `backbone` is \"unfrozen,\" and the entire model is trained.\n",
    "    * **LR:** A very low learning rate (`1e-5`) is used to slowly adapt the backbone features.\n",
    "* **Model Saving:** A `ModelCheckpoint` saves the model based on the **highest `Score (R^2)`** on the validation set, *not* the lowest loss. This is critical for capturing the model's peak performance (like the `R^2=0.64` spike at Epoch 11) and ignoring the unstable, overfitted epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6aac6fb",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-10T21:14:54.102313Z",
     "iopub.status.busy": "2025-11-10T21:14:54.102063Z",
     "iopub.status.idle": "2025-11-10T21:16:27.998228Z",
     "shell.execute_reply": "2025-11-10T21:16:27.997412Z"
    },
    "papermill": {
     "duration": 93.90086,
     "end_time": "2025-11-10T21:16:27.999560",
     "exception": false,
     "start_time": "2025-11-10T21:14:54.098700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/albumentations/check_version.py:147: UserWarning: Error fetching version info <urlopen error [Errno -3] Temporary failure in name resolution>\n",
      "  data = fetch_version_info()\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "================= Âä†ËΩΩÊµãËØïÊï∞ÊçÆ =================\n",
      "ÊâæÂà∞ 1 Âº†Áã¨Á´ãÊµãËØïÂõæÂÉè„ÄÇ\n",
      "\n",
      "================= ÈõÜÂêà A Êé®ÁêÜ =================\n",
      "\n",
      "================= Âä†ËΩΩÊ®°Âûã (5 Êäò) =================\n",
      "fold1 => variant=tiled_film, backbone=vit_base_patch14_reg4_dinov2, input_res=518\n",
      "tiled_film_best_model_fold2.pth => variant=tiled_film, backbone=vit_base_patch14_reg4_dinov2, input_res=518\n",
      "tiled_film_best_model_fold3.pth => variant=tiled_film, backbone=vit_base_patch14_reg4_dinov2, input_res=518\n",
      "tiled_film_best_model_fold4.pth => variant=tiled_film, backbone=vit_base_patch14_reg4_dinov2, input_res=518\n",
      "tiled_film_best_model_fold5.pth => variant=tiled_film, backbone=vit_base_patch14_reg4_dinov2, input_res=518\n",
      "\n",
      "--- TTA ËßÜËßí 1/3 (resize=518) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TTA ËßÜËßí 2/3 (resize=518) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TTA ËßÜËßí 3/3 (resize=518) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= ÈõÜÂêà B Êé®ÁêÜ =================\n",
      "\n",
      "================= Âä†ËΩΩÊ®°Âûã (5 Êäò) =================\n",
      "fold1 => variant=tiled_film, backbone=vit_base_patch14_reg4_dinov2, input_res=518\n",
      "tiled_film_best_model_fold2.pth => variant=tiled_film, backbone=vit_base_patch14_reg4_dinov2, input_res=518\n",
      "tiled_film_best_model_fold3.pth => variant=tiled_film, backbone=vit_base_patch14_reg4_dinov2, input_res=518\n",
      "tiled_film_best_model_fold4.pth => variant=tiled_film, backbone=vit_base_patch14_reg4_dinov2, input_res=518\n",
      "tiled_film_best_model_fold5.pth => variant=tiled_film, backbone=vit_base_patch14_reg4_dinov2, input_res=518\n",
      "\n",
      "--- TTA ËßÜËßí 1/3 (resize=518) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TTA ËßÜËßí 2/3 (resize=518) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TTA ËßÜËßí 3/3 (resize=518) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ Â∑≤ÁîüÊàêÊèê‰∫§Êñá‰ª∂: submission.csv\n",
      "                    sample_id     target\n",
      "0  ID1001187975__Dry_Clover_g   1.782589\n",
      "1    ID1001187975__Dry_Dead_g  26.787958\n",
      "2   ID1001187975__Dry_Green_g  32.905235\n",
      "3   ID1001187975__Dry_Total_g  61.475784\n",
      "4         ID1001187975__GDM_g  34.687828\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CSIRO Image2Biomass ‚Äî Dual-Ensemble Inference (A√ó0.88 + B√ó0.12)\n",
    "# - ÊØèÂ•ó‰∏∫ 5-fold ensemble + ÁÆÄÂçï TTAÔºàÂéüÂõæ/Ê∞¥Âπ≥ÁøªËΩ¨/ÂûÇÁõ¥ÁøªËΩ¨Ôºâ\n",
    "# - Ëá™Âä®‰ªéËÆ≠ÁªÉÊùÉÈáç‰∏≠Ââ•Á¶ª student.* Â≠êÊ†ëÔºàÂéªÂâçÁºÄÔºâÔºå‰∏¢ÂºÉËÆ≠ÁªÉÊúüÂ§öÊ®°ÊÄÅÈÉ®ÂàÜ\n",
    "# - ‰æùÊçÆÊùÉÈáçÊòØÂê¶ÂåÖÂê´ film_left/right Ëá™Âä®Âà§Êñ≠Âèò‰ΩìÔºàtiled_film / tiled / plainÔºâ\n",
    "# - Á©∑‰∏æ DINO ‰∏ªÂπ≤ÂêçÔºà‰∏éËÆ≠ÁªÉ‰∏ÄËá¥ÔºâÔºåstrict ÂåπÈÖçÊàêÂäüÊâç‰ΩøÁî®\n",
    "# - final = 0.88*final_A + 0.12*final_B -> submission.csv\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import timm\n",
    "\n",
    "# =============================== ÈÖçÁΩÆ =========================================\n",
    "class CFG:\n",
    "    # Êï∞ÊçÆË∑ØÂæÑÔºàKaggle ÈªòËÆ§Ôºâ\n",
    "    BASE_PATH = \"/kaggle/input/csiro-biomass\"\n",
    "    TEST_CSV = os.path.join(BASE_PATH, \"test.csv\")\n",
    "    TEST_IMAGE_DIR = os.path.join(BASE_PATH, \"test\")\n",
    "\n",
    "    # ‚Äî‚Äî ÈõÜÂêà AÔºàÁ¨¨‰∏Ä‰ªΩ‰ª£Á†ÅÔºâ‚Äî‚Äî\n",
    "    MODEL_DIR_A = \"/kaggle/input/m/gothamjocker/csiro/pytorch/default/6\"   # ‚Üê ÊîπÊàê‰Ω†ÁöÑÊï∞ÊçÆÈõÜË∑ØÂæÑ\n",
    "    CKPTS_A = [\n",
    "        os.path.join(MODEL_DIR_A, \"tiled_film_best_model_fold1.pth\"),\n",
    "        os.path.join(MODEL_DIR_A, \"tiled_film_best_model_fold2.pth\"),\n",
    "        os.path.join(MODEL_DIR_A, \"tiled_film_best_model_fold3.pth\"),\n",
    "        os.path.join(MODEL_DIR_A, \"tiled_film_best_model_fold4.pth\"),\n",
    "        os.path.join(MODEL_DIR_A, \"tiled_film_best_model_fold5.pth\"),\n",
    "    ]\n",
    "\n",
    "    # ‚Äî‚Äî ÈõÜÂêà BÔºàÁ¨¨‰∫å‰ªΩ‰ª£Á†ÅÔºâ‚Äî‚Äî\n",
    "    MODEL_DIR_B = \"/kaggle/input/m/gothamjocker/csiro/pytorch/default/11\"  # ‚Üê ÊîπÊàê‰Ω†ÁöÑÊï∞ÊçÆÈõÜË∑ØÂæÑ\n",
    "    CKPTS_B = [\n",
    "        os.path.join(MODEL_DIR_B, \"tiled_film_best_model_fold1.pth\"),\n",
    "        os.path.join(MODEL_DIR_B, \"tiled_film_best_model_fold2.pth\"),\n",
    "        os.path.join(MODEL_DIR_B, \"tiled_film_best_model_fold3.pth\"),\n",
    "        os.path.join(MODEL_DIR_B, \"tiled_film_best_model_fold4.pth\"),\n",
    "        os.path.join(MODEL_DIR_B, \"tiled_film_best_model_fold5.pth\"),\n",
    "    ]\n",
    "\n",
    "    # ËûçÂêàÊùÉÈáç\n",
    "    W_A = 0.965\n",
    "    W_B = 0.035\n",
    "\n",
    "    SUBMISSION_FILE = \"submission.csv\"\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    BATCH_SIZE = 1\n",
    "    NUM_WORKERS = 0\n",
    "\n",
    "    # ‰∏éËÆ≠ÁªÉ‰∏ÄËá¥\n",
    "    DROPOUT = 0.30\n",
    "    HIDDEN_RATIO = 0.25\n",
    "    GRID = (2, 2)  # tiled / tiled_film ÁöÑÂàáÂùóÁΩëÊ†ºÔºàËã•ÊùÉÈáç‰∏çÊòØ tiled Á±ªÔºå‰πü‰ºöËá™Âä®ËØÜÂà´Ôºâ\n",
    "\n",
    "    # ËæìÂá∫ÂàóÈ°∫Â∫èÔºà‰∏éËÆ≠ÁªÉ‰∏ÄËá¥Ôºâ\n",
    "    ALL_TARGET_COLS = [\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\"]\n",
    "\n",
    "    # ËÆ≠ÁªÉÊó∂ÁöÑ DINO ÂÄôÈÄâÔºà‰ºòÂÖàÁ∫ß‰ªéÈ´òÂà∞‰ΩéÔºâ\n",
    "    DINO_CANDIDATES = [\n",
    "        \"vit_base_patch14_dinov3\",\n",
    "        \"vit_base_patch14_reg4_dinov3\",\n",
    "        \"vit_small_patch14_dinov3\",\n",
    "        \"vit_base_patch14_reg4_dinov2\",\n",
    "        \"vit_base_patch14_dinov2\",\n",
    "        \"vit_small_patch14_dinov2\",\n",
    "    ]\n",
    "\n",
    "print(f\"Device: {CFG.DEVICE}\")\n",
    "\n",
    "# =============================== Êï∞ÊçÆÈõÜÔºàÂ∑¶Âè≥‰∏§Ë∑ØÔºå‰∏éËÆ≠ÁªÉ‰∏ÄËá¥Ôºâ =================\n",
    "class TestBiomassDataset(Dataset):\n",
    "    def __init__(self, df, transform, image_dir):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.image_dir = image_dir\n",
    "        self.paths = self.df[\"image_path\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = os.path.basename(self.paths[idx])\n",
    "        full_path = os.path.join(self.image_dir, filename)\n",
    "        img = cv2.imread(full_path)\n",
    "        if img is None:\n",
    "            # ÂÆπÈîôÔºöËã•ËØªÂõæÂ§±Ë¥•ÔºåÁî®ÈªëÂõæÂç†‰Ωç\n",
    "            img = np.zeros((1000, 2000, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Â∑¶Âè≥ÂàáÂçäÔºàËÆ≠ÁªÉ/È™åËØÅÁªü‰∏ÄÁ≠ñÁï•Ôºâ\n",
    "        h, w, _ = img.shape\n",
    "        mid = w // 2\n",
    "        left = img[:, :mid]\n",
    "        right = img[:, mid:]\n",
    "\n",
    "        left_t = self.transform(image=left)[\"image\"]\n",
    "        right_t = self.transform(image=right)[\"image\"]\n",
    "        return left_t, right_t\n",
    "\n",
    "# =============================== ÊûÑÂª∫ DINO ‰∏ªÂπ≤ÔºàÊé®ÁêÜÁ´ØÔºâ ======================\n",
    "def _infer_input_res(m) -> int:\n",
    "    if hasattr(m, \"patch_embed\") and hasattr(m.patch_embed, \"img_size\"):\n",
    "        isz = m.patch_embed.img_size\n",
    "        return int(isz if isinstance(isz, (int, float)) else isz[0])\n",
    "    if hasattr(m, \"img_size\"):\n",
    "        isz = m.img_size\n",
    "        return int(isz if isinstance(isz, (int, float)) else isz[0])\n",
    "    dc = getattr(m, \"default_cfg\", {}) or {}\n",
    "    ins = dc.get(\"input_size\", None)\n",
    "    if ins:\n",
    "        if isinstance(ins, (tuple, list)) and len(ins) >= 2:\n",
    "            return int(ins[1])\n",
    "        return int(ins if isinstance(ins, (int, float)) else 224)\n",
    "    name = getattr(m, \"default_cfg\", {}).get(\"architecture\", \"\") or str(type(m))\n",
    "    return 518 if (\"dinov2\" in name.lower()) else 224\n",
    "\n",
    "def _build_dino_by_name(name: str):\n",
    "    # Êé®ÁêÜÁ´Ø‰∏çÈúÄË¶Å‰∏ãËΩΩÈ¢ÑËÆ≠ÁªÉÔºåÁõ¥Êé• pretrained=FalseÔºà‰ºöÁî®ÊùÉÈáçÂÆåÂÖ®Ë¶ÜÁõñÔºâ\n",
    "    m = timm.create_model(name, pretrained=False, num_classes=0)\n",
    "    feat = m.num_features\n",
    "    input_res = _infer_input_res(m)\n",
    "    return m, feat, input_res\n",
    "\n",
    "# =============================== Ê®°ÂûãÔºà‰∏éËÆ≠ÁªÉ‰∏•Ê†ºÂØπÈΩêÔºâ ========================\n",
    "class TwoStreamDINOBase(nn.Module):\n",
    "    def __init__(self, backbone_name: str, dropout: float = 0.3, hidden_ratio: float = 0.25):\n",
    "        super().__init__()\n",
    "        self.backbone, feat, input_res = _build_dino_by_name(backbone_name)\n",
    "        self.used_backbone_name = backbone_name\n",
    "        self.input_res = int(input_res)\n",
    "        self.feat_dim = feat\n",
    "        self.combined = feat * 2\n",
    "\n",
    "        hidden = max(8, int(self.combined * hidden_ratio))\n",
    "\n",
    "        def head():\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(self.combined, hidden),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden, 1),\n",
    "            )\n",
    "\n",
    "        self.head_green = head()\n",
    "        self.head_clover = head()\n",
    "        self.head_dead = head()\n",
    "        self.softplus = nn.Softplus(beta=1.0)\n",
    "\n",
    "    def _merge_heads(self, f_l: torch.Tensor, f_r: torch.Tensor):\n",
    "        f = torch.cat([f_l, f_r], dim=1)\n",
    "        green_pos = self.softplus(self.head_green(f))\n",
    "        clover_pos = self.softplus(self.head_clover(f))\n",
    "        dead_pos = self.softplus(self.head_dead(f))\n",
    "        gdm = green_pos + clover_pos\n",
    "        total = gdm + dead_pos\n",
    "        return total, gdm, green_pos\n",
    "\n",
    "class TwoStreamDINOPlain(TwoStreamDINOBase):\n",
    "    def forward(self, x_left: torch.Tensor, x_right: torch.Tensor):\n",
    "        f_l = self.backbone(x_left)\n",
    "        f_r = self.backbone(x_right)\n",
    "        return self._merge_heads(f_l, f_r)\n",
    "\n",
    "def _make_edges(L: int, parts: int):\n",
    "    step = L // parts\n",
    "    edges = []\n",
    "    start = 0\n",
    "    for _ in range(parts - 1):\n",
    "        edges.append((start, start + step))\n",
    "        start += step\n",
    "    edges.append((start, L))\n",
    "    return edges\n",
    "\n",
    "class TwoStreamDINOTiled(TwoStreamDINOBase):\n",
    "    def __init__(self, backbone_name: str, grid=(2, 2), **kwargs):\n",
    "        super().__init__(backbone_name, **kwargs)\n",
    "        self.grid = tuple(grid)\n",
    "\n",
    "    def _encode_tiles(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W = x.shape\n",
    "        r, c = self.grid\n",
    "        rows = _make_edges(H, r)\n",
    "        cols = _make_edges(W, c)\n",
    "        feats = []\n",
    "        for (rs, re) in rows:\n",
    "            for (cs, ce) in cols:\n",
    "                xt = x[:, :, rs:re, cs:ce]\n",
    "                if xt.shape[-2:] != (self.input_res, self.input_res):\n",
    "                    xt = F.interpolate(xt, size=(self.input_res, self.input_res), mode=\"bilinear\", align_corners=False)\n",
    "                ft = self.backbone(xt)\n",
    "                feats.append(ft)\n",
    "        feats = torch.stack(feats, dim=0).permute(1, 0, 2)  # (B, T, F)\n",
    "        feat_stream = feats.mean(dim=1)  # (B, F)\n",
    "        return feat_stream\n",
    "\n",
    "    def forward(self, x_left: torch.Tensor, x_right: torch.Tensor):\n",
    "        f_l = self._encode_tiles(x_left)\n",
    "        f_r = self._encode_tiles(x_right)\n",
    "        return self._merge_heads(f_l, f_r)\n",
    "\n",
    "class FiLM(nn.Module):\n",
    "    def __init__(self, in_dim: int):\n",
    "        super().__init__()\n",
    "        hid = max(32, in_dim // 2)\n",
    "        self.mlp = nn.Sequential(nn.Linear(in_dim, hid), nn.ReLU(inplace=True), nn.Linear(hid, in_dim * 2))\n",
    "\n",
    "    def forward(self, context: torch.Tensor):\n",
    "        gb = self.mlp(context)\n",
    "        gamma, beta = torch.chunk(gb, 2, dim=1)\n",
    "        return gamma, beta\n",
    "\n",
    "class TwoStreamDINOTiledFiLM(TwoStreamDINOBase):\n",
    "    def __init__(self, backbone_name: str, grid=(2, 2), **kwargs):\n",
    "        super().__init__(backbone_name, **kwargs)\n",
    "        self.grid = tuple(grid)\n",
    "        self.film_left = FiLM(self.feat_dim)\n",
    "        self.film_right = FiLM(self.feat_dim)\n",
    "\n",
    "    def _tiles_backbone(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W = x.shape\n",
    "        r, c = self.grid\n",
    "        rows = _make_edges(H, r)\n",
    "        cols = _make_edges(W, c)\n",
    "        feats = []\n",
    "        for (rs, re) in rows:\n",
    "            for (cs, ce) in cols:\n",
    "                xt = x[:, :, rs:re, cs:ce]\n",
    "                if xt.shape[-2:] != (self.input_res, self.input_res):\n",
    "                    xt = F.interpolate(xt, size=(self.input_res, self.input_res), mode=\"bilinear\", align_corners=False)\n",
    "                ft = self.backbone(xt)\n",
    "                feats.append(ft)\n",
    "        feats = torch.stack(feats, dim=0).permute(1, 0, 2)  # (B, T, F)\n",
    "        return feats\n",
    "\n",
    "    def _encode_stream(self, x: torch.Tensor, film: FiLM) -> torch.Tensor:\n",
    "        tiles = self._tiles_backbone(x)  # (B, T, F)\n",
    "        context = tiles.mean(dim=1)      # (B, F)\n",
    "        gamma, beta = film(context)      # (B, F)\n",
    "        tiles = tiles * (1 + gamma.unsqueeze(1)) + beta.unsqueeze(1)\n",
    "        feat_stream = tiles.mean(dim=1)  # (B, F)\n",
    "        return feat_stream\n",
    "\n",
    "    def forward(self, x_left: torch.Tensor, x_right: torch.Tensor):\n",
    "        f_l = self._encode_stream(x_left, self.film_left)\n",
    "        f_r = self._encode_stream(x_right, self.film_right)\n",
    "        return self._merge_heads(f_l, f_r)\n",
    "\n",
    "# =============================== ÊùÉÈáçÊ∏ÖÊ¥ó‰∏éÂä†ËΩΩ =================================\n",
    "def _strip_module_prefix(sd: dict):\n",
    "    if not sd:\n",
    "        return sd\n",
    "    keys = list(sd.keys())\n",
    "    if all(k.startswith(\"module.\") for k in keys):\n",
    "        return {k[len(\"module.\") :]: v for k, v in sd.items()}\n",
    "    return sd\n",
    "\n",
    "def _extract_student_substate(sd: dict) -> dict:\n",
    "    \"\"\"\n",
    "    ËÆ≠ÁªÉ‰øùÂ≠òÁöÑÊòØ MultiModalStudentTeacher ÁöÑ state_dict„ÄÇ\n",
    "    ËøôÈáåÂâ•Á¶ªÔºö\n",
    "      - ÂÖàÂéª 'module.' ÂâçÁºÄ\n",
    "      - Ëã•Â≠òÂú® 'student.' ÂâçÁºÄÔºå‰ªÖ‰øùÁïôËØ•Â≠êÊ†ëÂπ∂ÂéªÊéâ 'student.' ÂâçÁºÄ\n",
    "      - ‰∏¢ÂºÉ‰ªÖËÆ≠ÁªÉÊúüÁöÑÂ§öÊ®°ÊÄÅ/Ëí∏È¶èÁõ∏ÂÖ≥Â±ÇÔºàtxt_enc.*, img_proj.*, txt_film_left.*, txt_film_right.*Ôºâ\n",
    "    \"\"\"\n",
    "    sd = _strip_module_prefix(sd)\n",
    "    has_student = any(k.startswith(\"student.\") for k in sd.keys())\n",
    "    if has_student:\n",
    "        sd = {k[len(\"student.\") :]: v for k, v in sd.items() if k.startswith(\"student.\")}\n",
    "    drop_prefixes = (\"txt_enc.\", \"img_proj.\", \"txt_film_left.\", \"txt_film_right.\")\n",
    "    sd = {k: v for k, v in sd.items() if not k.startswith(drop_prefixes)}\n",
    "    return sd\n",
    "\n",
    "def _has_film(sd_keys: set) -> bool:\n",
    "    return any(k.startswith(\"film_left.mlp.\") for k in sd_keys) or any(k.startswith(\"film_right.mlp.\") for k in sd_keys)\n",
    "\n",
    "def _try_build_and_load(sd: dict, backbone_name: str, variant: str, grid=(2, 2)):\n",
    "    if variant == \"tiled_film\":\n",
    "        model = TwoStreamDINOTiledFiLM(backbone_name, grid=grid, dropout=CFG.DROPOUT, hidden_ratio=CFG.HIDDEN_RATIO)\n",
    "    elif variant == \"tiled\":\n",
    "        model = TwoStreamDINOTiled(backbone_name, grid=grid, dropout=CFG.DROPOUT, hidden_ratio=CFG.HIDDEN_RATIO)\n",
    "    else:\n",
    "        model = TwoStreamDINOPlain(backbone_name, dropout=CFG.DROPOUT, hidden_ratio=CFG.HIDDEN_RATIO)\n",
    "\n",
    "    # ‰∏•Ê†ºÂåπÈÖçÔºömissing/unexpected ÈÉΩ‰∏∫ 0\n",
    "    result = model.load_state_dict(sd, strict=False)\n",
    "    missing = getattr(result, \"missing_keys\", [])\n",
    "    unexpected = getattr(result, \"unexpected_keys\", [])\n",
    "    if len(missing) == 0 and len(unexpected) == 0:\n",
    "        model.to(CFG.DEVICE)\n",
    "        model.eval()\n",
    "        return model\n",
    "    return None\n",
    "\n",
    "def load_fold_model_auto(path: str, grid=(2, 2)):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    # ÂÖºÂÆπ PyTorch 2.6 ÁöÑ weights_only ÂèòÊõ¥\n",
    "    try:\n",
    "        raw_sd = torch.load(path, map_location=CFG.DEVICE, weights_only=True)\n",
    "    except TypeError:\n",
    "        raw_sd = torch.load(path, map_location=CFG.DEVICE)\n",
    "    sd = _extract_student_substate(raw_sd)\n",
    "    keys = set(sd.keys())\n",
    "\n",
    "    # Âà§ÂÆöÊòØÂê¶ FiLM\n",
    "    is_film = _has_film(keys)\n",
    "    variant_order = [\"tiled_film\"] if is_film else [\"tiled\", \"plain\"]\n",
    "\n",
    "    # Á©∑‰∏æ‰∏ªÂπ≤ + Âèò‰ΩìÔºåÁõ¥Âà∞‰∏•Ê†ºÂåπÈÖç\n",
    "    for variant in variant_order:\n",
    "        for backbone in CFG.DINO_CANDIDATES:\n",
    "            try:\n",
    "                m = _try_build_and_load(sd, backbone, variant, grid=grid)\n",
    "                if m is not None:\n",
    "                    return m, variant, backbone\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    # Âà∞Ê≠§‰ªçÂ§±Ë¥•ÔºåÊèê‰æõËØäÊñ≠‰ø°ÊÅØ\n",
    "    raise RuntimeError(\n",
    "        f\"Êó†Ê≥ï‰∏∫ {os.path.basename(path)} ÊâæÂà∞ÂåπÈÖçÁöÑÂèò‰Ωì/‰∏ªÂπ≤„ÄÇ\"\n",
    "        f\" Ê£ÄÊµãÂà∞ {'tiled_film' if is_film else 'non-film'} ÊùÉÈáçÔºåËØ∑Ê£ÄÊü•ËÆ≠ÁªÉ-Êé®ÁêÜ‰∏ÄËá¥ÊÄß„ÄÇ\"\n",
    "    )\n",
    "\n",
    "# =============================== TTA ÂèòÊç¢ÔºàÂä®ÊÄÅÊåâÊ®°Âûã input_resÔºâ ===============\n",
    "def get_tta_transforms(img_size: int):\n",
    "    base = [A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()]\n",
    "    original = A.Compose([A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA), *base])\n",
    "    hflip = A.Compose([A.HorizontalFlip(p=1.0), A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA), *base])\n",
    "    vflip = A.Compose([A.VerticalFlip(p=1.0), A.Resize(img_size, img_size, interpolation=cv2.INTER_AREA), *base])\n",
    "    return [original, hflip, vflip]\n",
    "\n",
    "# =============================== Êé®ÁêÜÔºàÁªü‰∏Ä‰∫ßÂá∫ 5 ÁõÆÊ†áÔºâ =======================\n",
    "@torch.no_grad()\n",
    "def predict_one_view(models, loader):\n",
    "    out_list = []\n",
    "    amp_dtype = \"cuda\" if CFG.DEVICE.type == \"cuda\" else \"cpu\"\n",
    "\n",
    "    for (xl, xr) in tqdm(loader, desc=\"  Predicting View\", leave=False):\n",
    "        xl = xl.to(CFG.DEVICE, non_blocking=True)\n",
    "        xr = xr.to(CFG.DEVICE, non_blocking=True)\n",
    "\n",
    "        per_model_preds = []\n",
    "        with torch.amp.autocast(amp_dtype, enabled=(CFG.DEVICE.type == \"cuda\")):\n",
    "            for m in models:\n",
    "                total, gdm, green = m(xl, xr)\n",
    "                dead = total - gdm\n",
    "                clover = gdm - green\n",
    "                five = torch.cat([green, dead, clover, gdm, total], dim=1)\n",
    "                five = torch.clamp(five, min=0.0)  # ÈùûË¥üÁ∫¶Êùü\n",
    "                per_model_preds.append(five.float().cpu())\n",
    "\n",
    "        stacked = torch.mean(torch.stack(per_model_preds, dim=0), dim=0)\n",
    "        out_list.append(stacked.numpy())\n",
    "\n",
    "    return np.concatenate(out_list, axis=0)\n",
    "\n",
    "def run_inference_for_ckpts(ckpt_list, test_unique, image_dir):\n",
    "    print(\"\\n================= Âä†ËΩΩÊ®°Âûã (5 Êäò) =================\")\n",
    "    models = []\n",
    "\n",
    "    # ÂÖàÁî®Á¨¨‰∏ÄÊäòÁ°ÆÂÆöËæìÂÖ•ÂàÜËæ®Áéá\n",
    "    m1, v1, b1 = load_fold_model_auto(ckpt_list[0], grid=CFG.GRID)\n",
    "    models.append(m1)\n",
    "    backbone_res = int(getattr(m1, \"input_res\", 518))\n",
    "    print(f\"fold1 => variant={v1}, backbone={b1}, input_res={backbone_res}\")\n",
    "\n",
    "    for p in ckpt_list[1:]:\n",
    "        m, v, b = load_fold_model_auto(p, grid=CFG.GRID)\n",
    "        print(f\"{os.path.basename(p)} => variant={v}, backbone={b}, input_res={getattr(m, 'input_res', '?')}\")\n",
    "        models.append(m)\n",
    "\n",
    "    # ÂáÜÂ§á TTA ËßÜËßí\n",
    "    tta_trans = get_tta_transforms(backbone_res)\n",
    "    per_view_preds = []\n",
    "    for i, t in enumerate(tta_trans):\n",
    "        print(f\"\\n--- TTA ËßÜËßí {i+1}/{len(tta_trans)} (resize={backbone_res}) ---\")\n",
    "        ds = TestBiomassDataset(test_unique, t, image_dir)\n",
    "        dl = DataLoader(ds, batch_size=CFG.BATCH_SIZE, shuffle=False, num_workers=CFG.NUM_WORKERS, pin_memory=True)\n",
    "        view_5 = predict_one_view(models, dl)  # [N,5]\n",
    "        per_view_preds.append(view_5)\n",
    "\n",
    "    # TTA Âπ≥Âùá\n",
    "    final_5 = np.mean(per_view_preds, axis=0)  # [N,5]: [green, dead, clover, gdm, total]\n",
    "    return final_5\n",
    "\n",
    "def run_dual_ensembles_and_fuse():\n",
    "    print(\"\\n================= Âä†ËΩΩÊµãËØïÊï∞ÊçÆ =================\")\n",
    "    test_long = pd.read_csv(CFG.TEST_CSV)\n",
    "    test_unique = test_long.drop_duplicates(subset=[\"image_path\"]).reset_index(drop=True)\n",
    "    print(f\"ÊâæÂà∞ {len(test_unique)} Âº†Áã¨Á´ãÊµãËØïÂõæÂÉè„ÄÇ\")\n",
    "\n",
    "    print(\"\\n================= ÈõÜÂêà A Êé®ÁêÜ =================\")\n",
    "    final_a = run_inference_for_ckpts(CFG.CKPTS_A, test_unique, CFG.TEST_IMAGE_DIR)\n",
    "\n",
    "    print(\"\\n================= ÈõÜÂêà B Êé®ÁêÜ =================\")\n",
    "    final_b = run_inference_for_ckpts(CFG.CKPTS_B, test_unique, CFG.TEST_IMAGE_DIR)\n",
    "\n",
    "    assert final_a.shape == final_b.shape, \"A/B ‰∏§‰∏™ÁªìÊûúÂΩ¢Áä∂‰∏ç‰∏ÄËá¥ÔºåÊó†Ê≥ïÂä†ÊùÉËûçÂêà„ÄÇ\"\n",
    "    final = CFG.W_A * final_a + CFG.W_B * final_b\n",
    "    return final, test_long, test_unique\n",
    "\n",
    "# =============================== ÁîüÊàêÊèê‰∫§ ======================================\n",
    "def create_submission(final_5, test_long, test_unique):\n",
    "    green = final_5[:, 0]\n",
    "    dead = final_5[:, 1]\n",
    "    clover = final_5[:, 2]\n",
    "    gdm = final_5[:, 3]\n",
    "    total = final_5[:, 4]\n",
    "\n",
    "    # ÊúÄÁªàÂÜçÂÅö‰∏ÄÊ¨°ÈùûË¥üË£ÅÂâ™‰∏é NaN/Inf Â§ÑÁêÜ\n",
    "    def nnz(x):\n",
    "        x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    green, dead, clover, gdm, total = map(nnz, [green, dead, clover, gdm, total])\n",
    "\n",
    "    wide = pd.DataFrame(\n",
    "        {\n",
    "            \"image_path\": test_unique[\"image_path\"],\n",
    "            \"Dry_Green_g\": green,\n",
    "            \"Dry_Dead_g\": dead,\n",
    "            \"Dry_Clover_g\": clover,\n",
    "            \"GDM_g\": gdm,\n",
    "            \"Dry_Total_g\": total,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    long_preds = wide.melt(\n",
    "        id_vars=[\"image_path\"],\n",
    "        value_vars=CFG.ALL_TARGET_COLS,\n",
    "        var_name=\"target_name\",\n",
    "        value_name=\"target\",\n",
    "    )\n",
    "\n",
    "    sub = pd.merge(\n",
    "        test_long[[\"sample_id\", \"image_path\", \"target_name\"]],\n",
    "        long_preds,\n",
    "        on=[\"image_path\", \"target_name\"],\n",
    "        how=\"left\",\n",
    "    )[[\"sample_id\", \"target\"]]\n",
    "\n",
    "    sub[\"target\"] = np.nan_to_num(sub[\"target\"], nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    sub.to_csv(CFG.SUBMISSION_FILE, index=False)\n",
    "    print(f\"\\nüéâ Â∑≤ÁîüÊàêÊèê‰∫§Êñá‰ª∂: {CFG.SUBMISSION_FILE}\")\n",
    "    print(sub.head())\n",
    "    return sub\n",
    "\n",
    "# =============================== ÂÖ•Âè£ =========================================\n",
    "if __name__ == \"__main__\":\n",
    "    final_5, df_long, df_unique = run_dual_ensembles_and_fuse()\n",
    "    _ = create_submission(final_5, df_long, df_unique)\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd4a2b1",
   "metadata": {
    "papermill": {
     "duration": 0.003338,
     "end_time": "2025-11-10T21:16:28.006679",
     "exception": false,
     "start_time": "2025-11-10T21:16:28.003341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "isSourceIdPinned": false,
     "sourceId": 112509,
     "sourceType": "competition"
    },
    {
     "datasetId": 8644657,
     "sourceId": 13603982,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 272104372,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 272531615,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 487624,
     "modelInstanceId": 471723,
     "sourceId": 638209,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 487624,
     "modelInstanceId": 471723,
     "sourceId": 630489,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 100.055839,
   "end_time": "2025-11-10T21:16:30.285542",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-10T21:14:50.229703",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
